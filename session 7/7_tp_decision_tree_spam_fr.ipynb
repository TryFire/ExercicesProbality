{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lors de ce TP, nous allons explorer la notion d'information mutuelle dans le cadre d'une tâche de classification de mails :  un mail est-il un spam ou un ham  ? \n",
    "\n",
    "Pour manipuler les arbres de décision et les données, nous allons utiliser sklearn (ou scikit-learn) dont la page web est : http://scikit-learn.org/stable/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Les données\n",
    "\n",
    "Récupérer les données (lingspam.p.zip) et décompresser l'archive. L'archive est un pickle des données déjà préparées. \n",
    "Voici du code pour lire et mettre en forme les donnéés (avec les imports aussi): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os \n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = pickle.load(open(\"lingspam.p\", \"rb\" ), encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- De quels types sont les variables  train_texts, test_texts, train_labels, test_labels ? \n",
    "- leurs dimensions ? \n",
    "- De quels types sont les données stockées ? \n",
    "- Combien y-a-t-il d'exemple pour l'apprentissage et le test  ?\n",
    "\n",
    "## Matrice terme/document \n",
    "\n",
    "Nous allons maintenant transformer les données en utilisant sklearn. Pour cela il existe un outil: CountVectorizer\n",
    "Cet objet permet de générer une matrice de compte. Nous allons utiliser la version binaire: chaque caractéristique est binaire, indépendante des autres caractéristique et représente la présence (ou l'absence) d'un mot dans un mail.  Pour cela nous procédons en 2 étapes: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(2604,) (2604,) (290,) (290,)\n",
      "b'Subject: pragmatics / extended deadline\\n\\nextended deadline - december 1 , 1998 : call for papers pragma99 international pragmatics conference on pragmatics and negotiation june 13-16 , 1999 tel aviv university and hebrew university of jerusalem tel aviv and jerusalem israel the main theme of this conference is the pragmatics of negotiation , interpreted in a very broad sense . interlocutors engage in negotiations about every aspect of their interaction - such as floor access and topic selection , contextual assumptions , conversational goals , and the ( mis ) interpretation and repair of their messages . topics such as cross-cultural and cross-gender ( mis ) communications , conversational procedures in disputes and collaborations , argumentation practices , and effects of assumptions and goals on the negotiating strategies of interlocutors are of special interest for this conference . the conference will be interdisciplinary , bringing together pragmaticists , linguists , philosophers , anthropologists , sociologists and political scientists . we are soliciting papers on all issues relevant to the theme of the conference , as well as papers in other areas of pragmatics and dialogue analysis . the conference will include plenary addresses , regular session lectures , and organized panels around any of the relevant topics . among the plenary speakers : elinor ochs ( ucla ) , itamar rabinovitch ( tel aviv university ) , emanual schegloff ( ucla ) , thomas schelling ( university of maryland ) , deborah schiffrin ( georgetown university ) , deborah tannen ( georgetown university ) , ruth wodak ( university of vienna ) . presentation of regular session lectures is 30 minutes long , with a subsequent discussion of 10 minutes . panels take the form of a series of closely related lectures on a specific topic , which may or may not be directly related to the special topic of the conference . they may consist of one , two or three units of 120 minutes . within each panel unit a maximum of four 20 - minute presentations are given consecutively , followed by a minimum of 30 minutes of discussion ( either devoted entirely to an open discussion , or taken up in part by comments by a discussant or discussants ) . panels are composed of contributions attracted by panel organizers , combined with individually submitted papers when judged appropriate by the program committee in consultation with the panel organizers . typically , written versions or extensive outlines of all panel contributions should be available before the conference to facilitate discussion . submissions abstracts for papers and panels should be submitted in the following format : 1 . for papers - five copies of an anonymous abstract ( up to 300 words ) . 2 . for panels - a preliminary proposal of one page , detailing title , area of interest , name of organizer ( s ) and invited participants to be sent by sept . 30 , 1998 . organizers of approved panels will then be invited to submit a full set of abstracts , including : a . a brief description of the topic area , b . a list of participants ( with full details , see below ) , c . abstracts by each of the participants by november 1 , 1998 . 3 . in all cases , a page stating : a . title , b . audiovisual / computer request , and c . for each author : i . full name and affiliation ; ii . current address ; iii . e - mail address ; iv . fax number . deadline for submission of abstracts : dec . 1 , 1998 . abstracts may be sent by hard copy , disk , or e-mail to pragma99 , faculty of humanities , tel aviv university , tel aviv 69978 , israel . e - mail : pragma99 @ post . tau . ac . il date of notification : march 1 , 1999 . program committee : mira ariel , hava bat - zeev shyldkrot , jonathan berg , anat biletzki , shoshana blum - kulka , marcelo dascal , nomi erteschik - shir , tamar katriel , ruth manor , george - elia sarfati , elda weizman , yael ziv . = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = pragma99 registration form please send the following information , accompanied by cheque payable to tel - aviv university in the amount of us $ 75 if paid before november 1 , 1998 , otherwise us $ 100 , to pragma99 faculty of humanities tel aviv university tel aviv 69978 , israel dr . / mr . / mrs . / ms . / name : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ address : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ university / organization : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ email : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ fax : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ( home ) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ( office ) telephone : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ( home ) _ _ _ _ _ _ _ _ _ _ _ _ _ ( office ) signature : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ date : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ those wishing to pay by credit card should provide the following information : type of credit card : mastercard / visa / american express name as it appears on credit card : sum of paymnt : us $ _ _ _ _ _ _ _ _ _ _ card no . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ expiration date : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ date : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ signature : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ those wishing to present a paper should follow the instructions above . hotel information will be provided after registration . the international association for dialogue analysis is co-sponsoring a part of our conference , which will be devoted to \" negotiation as a dialogic concept . \" for further information , contact edda weigand ( e-mail : weigand @ uni-muenster . de ) . = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = [ forms can also be returned by fax to 972 - 3-6407839 , or by e-mail to pragma99 @ post . tau . ac . il . ]\\n'\n",
      "[1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "les type des variables train_texts, test_texts, train_labels, test_labels sont <class 'numpy.ndarray'>\n",
    "\n",
    "leurs dimensions sont (2604,) (2604,) (290,) (290,)\n",
    "\n",
    "type de train_texts et train_labels est contents de e-mail\n",
    "\n",
    "type de train_labels et test_labels est valeur binaire\n",
    "\n",
    "'''\n",
    "\n",
    "print(type(train_texts), type(train_labels))\n",
    "print(train_texts.shape, train_labels.shape, test_texts.shape, test_labels.shape)\n",
    "print(train_texts[0])\n",
    "print(train_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "x_train = cv.fit_transform(train_texts) # étape 1 \n",
    "x_test = cv.transform(test_texts) # étape 2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarder la documentation de sklearn sur CountVectorizer et répondre aux questions suivantes\n",
    "\n",
    "## Questions sur les données (refresher)\n",
    "\n",
    "- Comment expliquer ces deux étapes ? \n",
    "- Combien y-a-t-il de document dans les données de train ? \n",
    "- Combien y-a-t-il de document dans les données de test ? \n",
    "- Quel est le vocabulaire ? \n",
    "- Afficher le vecteur associé au premier document d'apprentissage? \n",
    "- Quel est le type de l'objet x_train ? Expliquer ce choix ? \n",
    "- Comment trouver le lien entre un mot et son indice ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1 etape: learn the vocabulary dictionary and return ter-document matrix\n",
    "2 etape: extract token counts out of raw text documents using the vocabulary fitted with fit_transform(or the one provided to the constructor)\n",
    "\n",
    "57752 de document dans les donne de train et test\n",
    "\n",
    "cv.get_feature_names()\n",
    "\n",
    "0 et 1, la parametre 'binary' est met par True, donc all non zero counts are set to 1\n",
    "\n",
    "l'objet x_train est scipy.sparse.csr.csr_matrix\n",
    "\n",
    "x_train.toarray() ou 1 intique que la ligne a le mot qui est dans la meme indice de feature_names\n",
    "\n",
    "'''\n",
    "type(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arbre de décision\n",
    "\n",
    "La classe pour apprendre un arbre de décision est DecisionTreeClassifier. Regarder la documentation de cette classe et commenter la ligne suivante: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "criterion: the function to measure the quality of a split, \"gini\" or \"entropy\" here is \"entropy\" for the information gain\n",
    "\n",
    "splitter: the strategy used to choose the split at each node, random or best \n",
    "\n",
    "random_state: the seed used by the random number generator, this has to be fixed to obtain a deterministic behaviour during fitting\n",
    "'''\n",
    "\n",
    "classifier = DecisionTreeClassifier(criterion=\"entropy\", splitter='best', random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- apprendre l'arbre de décision sur les données d'apprentissage\n",
    "- comment connaitre la prédiction du modèle pour le premier document de test ? \n",
    "- la réponse du modèle est-elle correcte ? \n",
    "- Calculer le score de bonne prédiction sur les données de test. \n",
    "- Afficher les 10  caractéristiques sélectionnnés comme les plus importantes lors de la contruction de l'arbre de décision. \n",
    "- Regarder l'importance pour toutes les caractéristiques, que constatez-vous ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apprendre l'arbre de décision sur les données d'apprentissage:\n",
    "classifier.fit(x_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "# for show the tree constructed by DecisionTreeClassifier\n",
    "from collections import Counter\n",
    "def showTree(dt, dictionary):\n",
    "    left   = dt.tree_.children_left\n",
    "    right  = dt.tree_.children_right\n",
    "    thresh = dt.tree_.threshold\n",
    "    feats  = [ dictionary[i] for i in dt.tree_.feature ]\n",
    "    value  = dt.tree_.value\n",
    "    def showTree_(node, s, depth):\n",
    "        for i in range(depth-1):\n",
    "            sys.stdout.write('|    ')\n",
    "        if depth > 0:\n",
    "            sys.stdout.write('-')\n",
    "            sys.stdout.write(s)\n",
    "            sys.stdout.write('-> ')\n",
    "        if thresh[node] == -2: # leaf\n",
    "            print('class {}\\t({} for class 0, {} for class 1)'.format(np.argmax(value[node]), value[node][0,0], value[node][0,1]))\n",
    "        else: # internal node\n",
    "            print('{}?'.format(feats[node]))\n",
    "            showTree_(left[ node], 'N', depth+1)\n",
    "            showTree_(right[node], 'Y', depth+1)\n",
    "\n",
    "    showTree_(0, '', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language?\n",
      "-N-> free?\n",
      "|    -N-> you?\n",
      "|    |    -N-> share?\n",
      "|    |    |    -N-> click?\n",
      "|    |    |    |    -N-> visit?\n",
      "|    |    |    |    |    -N-> exclusive?\n",
      "|    |    |    |    |    |    -N-> interviews?\n",
      "|    |    |    |    |    |    |    -N-> xxx?\n",
      "|    |    |    |    |    |    |    |    -N-> equitybuilder?\n",
      "|    |    |    |    |    |    |    |    |    -N-> gino?\n",
      "|    |    |    |    |    |    |    |    |    |    -N-> class 1\t(0.0 for class 0, 536.0 for class 1)\n",
      "|    |    |    |    |    |    |    |    |    |    -Y-> class 0\t(1.0 for class 0, 0.0 for class 1)\n",
      "|    |    |    |    |    |    |    |    |    -Y-> class 0\t(1.0 for class 0, 0.0 for class 1)\n",
      "|    |    |    |    |    |    |    |    -Y-> class 0\t(1.0 for class 0, 0.0 for class 1)\n",
      "|    |    |    |    |    |    |    -Y-> but?\n",
      "|    |    |    |    |    |    |    |    -N-> class 0\t(2.0 for class 0, 0.0 for class 1)\n",
      "|    |    |    |    |    |    |    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    |    |    |    |    |    -Y-> authors?\n",
      "|    |    |    |    |    |    |    -N-> class 0\t(2.0 for class 0, 0.0 for class 1)\n",
      "|    |    |    |    |    |    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    |    |    |    |    -Y-> university?\n",
      "|    |    |    |    |    |    -N-> or?\n",
      "|    |    |    |    |    |    |    -N-> class 0\t(5.0 for class 0, 0.0 for class 1)\n",
      "|    |    |    |    |    |    |    -Y-> class 1\t(0.0 for class 0, 3.0 for class 1)\n",
      "|    |    |    |    |    |    -Y-> class 1\t(0.0 for class 0, 15.0 for class 1)\n",
      "|    |    |    |    -Y-> following?\n",
      "|    |    |    |    |    -N-> class 0\t(5.0 for class 0, 0.0 for class 1)\n",
      "|    |    |    |    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    |    |    -Y-> price?\n",
      "|    |    |    |    -N-> class 1\t(0.0 for class 0, 3.0 for class 1)\n",
      "|    |    |    |    -Y-> class 0\t(7.0 for class 0, 0.0 for class 1)\n",
      "|    |    -Y-> linguistics?\n",
      "|    |    |    -N-> edu?\n",
      "|    |    |    |    -N-> english?\n",
      "|    |    |    |    |    -N-> research?\n",
      "|    |    |    |    |    |    -N-> your?\n",
      "|    |    |    |    |    |    |    -N-> http?\n",
      "|    |    |    |    |    |    |    |    -N-> click?\n",
      "|    |    |    |    |    |    |    |    |    -N-> remove?\n",
      "|    |    |    |    |    |    |    |    |    |    -N-> call?\n",
      "|    |    |    |    |    |    |    |    |    |    |    -N-> springfield?\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    -N-> class 1\t(0.0 for class 0, 37.0 for class 1)\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    -Y-> class 0\t(1.0 for class 0, 0.0 for class 1)\n",
      "|    |    |    |    |    |    |    |    |    |    |    -Y-> as?\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    -N-> class 0\t(4.0 for class 0, 0.0 for class 1)\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    |    |    |    |    |    |    |    |    |    -Y-> class 0\t(4.0 for class 0, 0.0 for class 1)\n",
      "|    |    |    |    |    |    |    |    |    -Y-> class 0\t(6.0 for class 0, 0.0 for class 1)\n",
      "|    |    |    |    |    |    |    |    -Y-> fax?\n",
      "|    |    |    |    |    |    |    |    |    -N-> via?\n",
      "|    |    |    |    |    |    |    |    |    |    -N-> class 0\t(29.0 for class 0, 0.0 for class 1)\n",
      "|    |    |    |    |    |    |    |    |    |    -Y-> class 1\t(0.0 for class 0, 2.0 for class 1)\n",
      "|    |    |    |    |    |    |    |    |    -Y-> class 1\t(0.0 for class 0, 4.0 for class 1)\n",
      "|    |    |    |    |    |    |    -Y-> de?\n",
      "|    |    |    |    |    |    |    |    -N-> official?\n",
      "|    |    |    |    |    |    |    |    |    -N-> nl?\n",
      "|    |    |    |    |    |    |    |    |    |    -N-> suggestions?\n",
      "|    |    |    |    |    |    |    |    |    |    |    -N-> class 0\t(106.0 for class 0, 0.0 for class 1)\n",
      "|    |    |    |    |    |    |    |    |    |    |    -Y-> pubcdrom?\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    -N-> class 1\t(0.0 for class 0, 2.0 for class 1)\n",
      "|    |    |    |    |    |    |    |    |    |    |    |    -Y-> class 0\t(1.0 for class 0, 0.0 for class 1)\n",
      "|    |    |    |    |    |    |    |    |    |    -Y-> class 1\t(0.0 for class 0, 2.0 for class 1)\n",
      "|    |    |    |    |    |    |    |    |    -Y-> mailing?\n",
      "|    |    |    |    |    |    |    |    |    |    -N-> class 1\t(0.0 for class 0, 3.0 for class 1)\n",
      "|    |    |    |    |    |    |    |    |    |    -Y-> class 0\t(1.0 for class 0, 0.0 for class 1)\n",
      "|    |    |    |    |    |    |    |    -Y-> class 1\t(0.0 for class 0, 5.0 for class 1)\n",
      "|    |    |    |    |    |    -Y-> chosen?\n",
      "|    |    |    |    |    |    |    -N-> class 1\t(0.0 for class 0, 20.0 for class 1)\n",
      "|    |    |    |    |    |    |    -Y-> class 0\t(1.0 for class 0, 0.0 for class 1)\n",
      "|    |    |    |    |    -Y-> ll?\n",
      "|    |    |    |    |    |    -N-> class 1\t(0.0 for class 0, 40.0 for class 1)\n",
      "|    |    |    |    |    |    -Y-> some?\n",
      "|    |    |    |    |    |    |    -N-> class 0\t(2.0 for class 0, 0.0 for class 1)\n",
      "|    |    |    |    |    |    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    |    |    |    -Y-> remove?\n",
      "|    |    |    |    |    -N-> class 1\t(0.0 for class 0, 52.0 for class 1)\n",
      "|    |    |    |    |    -Y-> class 0\t(1.0 for class 0, 0.0 for class 1)\n",
      "|    |    |    -Y-> class 1\t(0.0 for class 0, 111.0 for class 1)\n",
      "|    -Y-> university?\n",
      "|    |    -N-> syntax?\n",
      "|    |    |    -N-> pp?\n",
      "|    |    |    |    -N-> role?\n",
      "|    |    |    |    |    -N-> ftp?\n",
      "|    |    |    |    |    |    -N-> your?\n",
      "|    |    |    |    |    |    |    -N-> seems?\n",
      "|    |    |    |    |    |    |    |    -N-> sending?\n",
      "|    |    |    |    |    |    |    |    |    -N-> class 0\t(34.0 for class 0, 0.0 for class 1)\n",
      "|    |    |    |    |    |    |    |    |    -Y-> information?\n",
      "|    |    |    |    |    |    |    |    |    |    -N-> class 0\t(4.0 for class 0, 0.0 for class 1)\n",
      "|    |    |    |    |    |    |    |    |    |    -Y-> class 1\t(0.0 for class 0, 2.0 for class 1)\n",
      "|    |    |    |    |    |    |    |    -Y-> class 1\t(0.0 for class 0, 2.0 for class 1)\n",
      "|    |    |    |    |    |    |    -Y-> class 0\t(206.0 for class 0, 0.0 for class 1)\n",
      "|    |    |    |    |    |    -Y-> class 1\t(0.0 for class 0, 2.0 for class 1)\n",
      "|    |    |    |    |    -Y-> class 1\t(0.0 for class 0, 2.0 for class 1)\n",
      "|    |    |    |    -Y-> class 1\t(0.0 for class 0, 3.0 for class 1)\n",
      "|    |    |    -Y-> class 1\t(0.0 for class 0, 6.0 for class 1)\n",
      "|    |    -Y-> anyone?\n",
      "|    |    |    -N-> class 1\t(0.0 for class 0, 41.0 for class 1)\n",
      "|    |    |    -Y-> about?\n",
      "|    |    |    |    -N-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    |    |    |    -Y-> class 0\t(3.0 for class 0, 0.0 for class 1)\n",
      "-Y-> remove?\n",
      "|    -N-> restoring?\n",
      "|    |    -N-> class 1\t(0.0 for class 0, 1271.0 for class 1)\n",
      "|    |    -Y-> class 0\t(1.0 for class 0, 0.0 for class 1)\n",
      "|    -Y-> how?\n",
      "|    |    -N-> class 1\t(0.0 for class 0, 3.0 for class 1)\n",
      "|    |    -Y-> class 0\t(3.0 for class 0, 0.0 for class 1)\n"
     ]
    }
   ],
   "source": [
    "showTree(classifier,cv.get_feature_names() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language : 0.263709\n",
      "free : 0.177927\n",
      "you : 0.077782\n",
      "university : 0.070938\n",
      "linguistics : 0.050770\n",
      "edu : 0.029252\n",
      "english : 0.027605\n",
      "your : 0.024322\n",
      "remove : 0.023290\n",
      "click : 0.020828\n"
     ]
    }
   ],
   "source": [
    "importrance = classifier.feature_importances_\n",
    "features = cv.get_feature_names()\n",
    "# show the first num important features \n",
    "def showImportantFeature(importrance, features, num = 10):\n",
    "    fea_import = []\n",
    "    for i in range(len(importrance)):\n",
    "        fea_import.append((features[i], importrance[i]))\n",
    "    fea_import = sorted(fea_import, key=lambda x:x[1], reverse=True)\n",
    "    for i in range(num):\n",
    "        print('%s : %f'%(fea_import[i][0], fea_import[i][1]))\n",
    "    return fea_import\n",
    "_ = showImportantFeature(importrance, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9586206896551724"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(x_test,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profondeur de l'arbre \n",
    "\n",
    "Effectuer un apprentissage, puis évaluer le modèle avec différentes profondeurs maximum d'arbre (1,2,5,10,15,20,100,None): \n",
    "- construire le classifieur\n",
    "- calculer son score sur le test\n",
    "Une profondeur maximum de *None*  signifie qu'il n'y a pas de limitation. \n",
    "- Que constatez vous ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score in test set : 0.827586 with the profondeur :  1\n",
      "score in test set : 0.917241 with the profondeur :  2\n",
      "score in test set : 0.931034 with the profondeur :  5\n",
      "score in test set : 0.962069 with the profondeur :  10\n",
      "score in test set : 0.958621 with the profondeur :  15\n",
      "score in test set : 0.958621 with the profondeur :  20\n",
      "score in test set : 0.958621 with the profondeur :  100\n",
      "score in test set : 0.958621 with the profondeur :  None\n"
     ]
    }
   ],
   "source": [
    "def profondeur(train_set, train_label, test_set, test_label,l):\n",
    "    for i in l:\n",
    "        c = DecisionTreeClassifier(criterion=\"entropy\", splitter='best', random_state = 0, max_depth=i)\n",
    "        c.fit(train_set, train_label)\n",
    "        print('score in test set : %f with the profondeur : '%(c.score(test_set, test_label)), i)\n",
    "profondeur(x_train, train_labels, x_test, test_labels, [1,2,5,10,15,20,100,None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration du modèle\n",
    "Partons du modèle dont la profondeur n'est pas fixée.\n",
    "- Quelle est la profondeur de l'arbre construit ? \n",
    "- Combien avait-il de features dans les données ? \n",
    "- Combien de features sont considérées par le modèle ? \n",
    "- Quels sont les seuils de décision appliqués pour ces features ? Commenter les résultats obtenus. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(classifier.max_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation de l'arbre\n",
    "Il est possible de générer une figure qui représente l'arbre ainsi: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import  export_graphviz\n",
    "export_graphviz(classifier, out_file = \"unNomDeFichier\", feature_names = cv.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis vous pouvez convertir le fichier en pdf grâce à la commande unix suivante : \n",
    " dot -Tpdf unNomDeFichier  -o maFigure.pdf\n",
    "\n",
    "- Visualiser l'arbre \n",
    "- Regarder les seuils de décision  appliqués ? Expliquer les valeurs obtenues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information mutuelle \n",
    "\n",
    "A partir des données d'apprentissage, calculer: \n",
    "- l'entropie de la variable Y (qui représente la classe)\n",
    "- l'information mutuelle entre Y et chaque caractéristique binaire\n",
    "- quels sont les 10 caractéristiques (mots) ayant l'information mutuelle la plus élevée. \n",
    "\n",
    "Remarque: Pour accéder facilement aux comptes, il suffit d'apprendre un Bayésien Naif de type Bernoulli avec sklearn. L'objet résultant contient les comptes. Recouper ces résultats avec ceux obtenus avec l'arbre de décision. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6349050956769284\n"
     ]
    }
   ],
   "source": [
    "proba_1 = sum(train_labels)/len(train_labels)\n",
    "proba_0 = 1 - proba_1\n",
    "\n",
    "entropy_y = 0 - (proba_0*np.log(proba_0)+proba_1*np.log(proba_1))\n",
    "print(entropy_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
